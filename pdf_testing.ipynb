{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 'Preface', 5]\n",
      "[1, 'Contents', 8]\n",
      "[1, '1 Introduction', 13]\n",
      "[2, '1.1 Classical Use of Parallelism', 13]\n",
      "[2, \"1.2 Parallelism in Today's Hardware\", 14]\n",
      "[2, '1.3 Basic Concepts', 16]\n",
      "[2, '1.4 Overview of the Book', 17]\n",
      "[1, '2 Parallel Computer Architecture', 20]\n",
      "[2, '2.1 Processor Architecture and Technology Trends', 20]\n",
      "[2, \"2.2 Flynn's Taxonomy of Parallel Architectures\", 24]\n",
      "[2, '2.3 Memory Organization of Parallel Computers', 25]\n",
      "[3, '2.3.1 Computers with Distributed Memory Organization', 26]\n",
      "[3, '2.3.2 Computers with Shared Memory Organization', 29]\n",
      "[3, '2.3.3 Reducing memory access times', 31]\n",
      "[2, '2.4 Thread-Level Parallelism', 35]\n",
      "[3, '2.4.1 Simultaneous Multithreading', 35]\n",
      "[3, '2.4.2 Energy Consumption of Processors', 36]\n",
      "[3, '2.4.3 Multicore Processors', 37]\n",
      "[3, '2.4.4 Architecture of Multicore Processors', 39]\n",
      "[3, '2.4.5 Example: Architecture of the Intel Core i7', 43]\n",
      "[2, '2.5 Interconnection Networks', 46]\n",
      "[3, '2.5.1 Properties of Interconnection Networks', 47]\n",
      "[3, '2.5.2 Direct Interconnection Networks', 49]\n",
      "[3, '2.5.3 Embeddings', 55]\n",
      "[3, '2.5.4 Dynamic Interconnection Networks', 58]\n",
      "[2, '2.6 Routing and Switching', 63]\n",
      "[3, '2.6.1 Routing Algorithms', 64]\n",
      "[3, '2.6.2 Routing in the Omega Network', 72]\n",
      "[3, '2.6.3 Switching', 74]\n",
      "[3, '2.6.4 Flow control mechanisms', 82]\n",
      "[2, '2.7 Caches and Memory Hierarchy', 83]\n",
      "[3, '2.7.1 Characteristics of Caches', 84]\n",
      "[3, '2.7.2 Write Policy', 92]\n",
      "[3, '2.7.3 Cache coherency', 94]\n",
      "[3, '2.7.4 Memory consistency', 102]\n",
      "[2, '2.8 Example: IBM Blue Gene supercomputer', 108]\n",
      "[2, '2.9 Exercises for Chapter 2', 111]\n",
      "[1, '3 Parallel Programming Models', 115]\n",
      "[2, '3.1 Models for parallel systems', 115]\n",
      "[2, '3.2 Parallelization of programs', 118]\n",
      "[2, '3.3 Levels of parallelism', 120]\n",
      "[3, '3.3.1 Parallelism at instruction level', 120]\n",
      "[3, '3.3.2 Data parallelism', 122]\n",
      "[3, '3.3.3 Loop parallelism', 124]\n",
      "[3, '3.3.4 Functional parallelism ', 126]\n",
      "[3, '3.3.5 Explicit and implicit representation of parallelism', 127]\n",
      "[3, '3.3.6 Parallel programming patterns', 130]\n",
      "[2, '3.4 SIMD Computations', 135]\n",
      "[3, '3.4.1 Execution of vector operations', 135]\n",
      "[3, '3.4.2 SIMD instructions', 137]\n",
      "[2, '3.5 Data distributions for arrays', 138]\n",
      "[3, '3.5.1 Data distribution for one-dimensional arrays', 139]\n",
      "[3, '3.5.2 Data distribution for two-dimensional arrays', 140]\n",
      "[3, '3.5.3 Parameterized data distribution', 142]\n",
      "[2, '3.6 Information exchange', 143]\n",
      "[3, '3.6.1 Shared variables', 143]\n",
      "[3, '3.6.2 Communication operations', 144]\n",
      "[2, '3.7 Parallel matrix-vector product', 151]\n",
      "[3, '3.7.1 Parallel computation of scalar products', 152]\n",
      "[3, '3.7.2 Parallel computation of the linear combinations', 155]\n",
      "[2, '3.8 Processes and Threads', 156]\n",
      "[3, '3.8.1 Processes', 158]\n",
      "[3, '3.8.2 Threads', 159]\n",
      "[3, '3.8.3 Synchronization mechanisms', 162]\n",
      "[3, '3.8.4 Developing efficient and correct thread programs', 166]\n",
      "[2, '3.9 Further parallel programming approaches', 168]\n",
      "[3, '3.9.1 Approaches for new parallel languages', 169]\n",
      "[3, '3.9.2 Transactional memory', 171]\n",
      "[2, '3.10 Exercices for Chapter 3', 174]\n",
      "[1, '4 Performance Analysis of Parallel Programs', 178]\n",
      "[2, '4.1 Performance Evaluation of Computer Systems', 179]\n",
      "[3, '4.1.1 Evaluation of CPU Performance', 179]\n",
      "[3, '4.1.2 MIPS and MFLOPS', 181]\n",
      "[3, '4.1.3 Performance of Processors with a Memory Hierarchy', 183]\n",
      "[3, '4.1.4 Benchmark Programs', 185]\n",
      "[2, '4.2 Performance Metrics for Parallel Programs', 188]\n",
      "[3, '4.2.1 Speedup and Efficiency', 189]\n",
      "[3, '4.2.2 Scalability of Parallel Programs', 192]\n",
      "[2, '4.3 Asymptotic Times for Global Communication', 193]\n",
      "[3, '4.3.1 Implementing Global Communication Operations', 195]\n",
      "[3, '4.3.2 Communications Operations on a Hypercube', 200]\n",
      "[2, '4.4 Analysis of Parallel Execution Times', 208]\n",
      "[3, '4.4.1 Parallel Scalar Product', 208]\n",
      "[3, '4.4.2 Parallel Matrix-vector Product', 210]\n",
      "[2, '4.5 Parallel Computational Models', 212]\n",
      "[3, '4.5.1 PRAM Model', 213]\n",
      "[3, '4.5.2 BSP Model', 216]\n",
      "[3, '4.5.3 LogP Model', 218]\n",
      "[2, '4.6 Loop Scheduling and Loop Tiling', 220]\n",
      "[3, '4.6.1 Loop Scheduling', 221]\n",
      "[3, '4.6.2 Loop Tiling', 229]\n",
      "[2, '4.7 Exercises for Chapter 4', 231]\n",
      "[1, '5 Message-Passing Programming', 236]\n",
      "[2, '5.1 Introduction to MPI', 237]\n",
      "[3, '5.1.1 MPI point-to-point communication', 239]\n",
      "[3, '5.1.2 Deadlocks with Point-to-point Communications', 243]\n",
      "[3, '5.1.3 Nonblocking Operations and Communication Modes', 246]\n",
      "[3, '5.1.4 Communication mode', 250]\n",
      "[2, '5.2 Collective Communication Operations', 252]\n",
      "[3, '5.2.1 Collective Communication in MPI', 252]\n",
      "[3, '5.2.2 Deadlocks with Collective Communication', 265]\n",
      "[2, '5.3 Process Groups and Communicators', 267]\n",
      "[3, '5.3.1 Process Groups in MPI', 268]\n",
      "[3, '5.3.2 Process Topologies', 273]\n",
      "[3, '5.3.3 Timings and aborting processes', 277]\n",
      "[2, '5.4 Introduction to MPI-2', 278]\n",
      "[3, '5.4.1 Dynamic Process Generation and Management', 278]\n",
      "[3, '5.4.2 One-sided communication', 281]\n",
      "[2, '5.5 Exercises for Chapter 5', 290]\n",
      "[1, '6 Thread Programming', 295]\n",
      "[2, '6.1 Programming with Pthreads', 295]\n",
      "[3, '6.1.1 Creating and Merging Threads', 297]\n",
      "[3, '6.1.2 Thread Coordination with Pthreads', 301]\n",
      "[3, '6.1.3 Condition Variables', 306]\n",
      "[3, '6.1.4 Extended Lock Mechanism', 312]\n",
      "[3, '6.1.5 One-Time Initialization', 314]\n",
      "[3, '6.1.6 Implementation of a Task Pool', 315]\n",
      "[3, '6.1.7 Parallelism by Pipelining', 318]\n",
      "[3, '6.1.8 Implementation of a Client-Server Model', 324]\n",
      "[3, '6.1.9 Thread Attributes and Cancelation', 329]\n",
      "[3, '6.1.10 Thread Scheduling with Pthreads', 335]\n",
      "[3, '6.1.11 Priority Inversion', 341]\n",
      "[3, '6.1.12 Thread-specific Data', 343]\n",
      "[2, '6.2 Java Threads', 345]\n",
      "[3, '6.2.1 Thread Generation in Java', 345]\n",
      "[3, '6.2.2 Synchronization of Java Threads', 349]\n",
      "[3, '6.2.3 Wait and Notify', 357]\n",
      "[3, '6.2.4 Extended Synchronization Patterns', 363]\n",
      "[3, '6.2.5 Thread Scheduling in Java', 367]\n",
      "[3, '6.2.6 Package java.util.concurrent', 369]\n",
      "[2, '6.3 OpenMP', 375]\n",
      "[3, '6.3.1 Compiler directives', 377]\n",
      "[3, '6.3.2 Execution environment routines', 385]\n",
      "[3, '6.3.3 Coordination and synchronization of threads', 385]\n",
      "[2, '6.4 Exercises for Chapter6', 391]\n",
      "[1, '7 General Purpose GPU Programming', 395]\n",
      "[2, '7.1 The Architecture of GPUs', 395]\n",
      "[2, '7.2 Introduction to CUDA Programming', 401]\n",
      "[2, '7.3 Synchronization and Shared Memory', 407]\n",
      "[2, '7.4 CUDA Thread Scheduling', 412]\n",
      "[2, '7.5 Efficient Memory Access and Tiling Technique', 414]\n",
      "[2, '7.6 Introduction to OpenCL', 420]\n",
      "[2, '7.7 Exercises for Chapter 7', 422]\n",
      "[1, '8 Algorithms for Systems of Linear Equations', 424]\n",
      "[2, '8.1 Gaussian Elimination', 425]\n",
      "[3, '8.1.1 Gaussian Elimination and LU Decomposition', 425]\n",
      "[3, '8.1.2 Parallel Row-Cyclic Implementation', 429]\n",
      "[3, '8.1.3 Parallel Implementation with Checkerboard Distribution', 432]\n",
      "[3, '8.1.4 Analysis of the Parallel Execution Time', 438]\n",
      "[2, '8.2 Direct Methods for Linear Systems with Banded Structure', 443]\n",
      "[3, '8.2.1 Discretization of the Poisson Equation', 443]\n",
      "[3, '8.2.2 Tridiagonal Systems', 448]\n",
      "[3, '8.2.3 Generalization to Banded Matrices', 461]\n",
      "[3, '8.2.4 Solving the Discretized Poisson Equation', 463]\n",
      "[2, '8.3 Iterative Methods for Linear Systems', 465]\n",
      "[3, '8.3.1 Standard Iteration Methods', 466]\n",
      "[3, '8.3.2 Parallel implementation of the Jacobi Iteration', 470]\n",
      "[3, '8.3.3 Parallel Implementation of the Gauss-Seidel Iteration', 472]\n",
      "[3, '8.3.4 Gauss-Seidel Iteration for Sparse Systems', 474]\n",
      "[3, '8.3.5 Red-black Ordering', 477]\n",
      "[2, '8.4 Conjugate Gradient Method', 483]\n",
      "[3, '8.4.1 Sequential CG method', 483]\n",
      "[3, '8.4.2 Parallel CG Method', 485]\n",
      "[2, '8.5 Cholesky Factorization for Sparse Matrices', 490]\n",
      "[3, '8.5.1 Sequential Algorithm', 490]\n",
      "[3, '8.5.2 Storage Scheme for Sparse Matrices', 496]\n",
      "[3, '8.5.3 Implementation for Shared Variables', 498]\n",
      "[2, '8.6 Exercises for Chapter 8', 503]\n",
      "[1, 'References', 507]\n",
      "[1, 'Index', 515]\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# Save bookmarks to a CSV file\\nwith open('bookmarks.csv', 'w', newline='', encoding='utf-8') as file:\\n    writer = csv.writer(file)\\n    # Write the header\\n    writer.writerow(['Chapter', 'Page'])\\n    # Write the bookmarks\\n    for title, page in bookmarks:\\n        writer.writerow([title, page])\\n        \""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import csv\n",
    "\n",
    "# Function to recursively extract bookmarks\n",
    "def extract_bookmarks(bookmarks):\n",
    "    outline = []\n",
    "    for item in bookmarks:\n",
    "        print(item)\n",
    "    return outline\n",
    "\n",
    "# Open the PDF file\n",
    "pdf_file = 'example2.pdf'  # specify the path to your PDF file\n",
    "doc = fitz.open(pdf_file)\n",
    "\n",
    "# Extract the bookmarks\n",
    "bookmarks = extract_bookmarks(doc.get_toc())\n",
    "\n",
    "print(bookmarks)\n",
    "\"\"\"\n",
    "# Save bookmarks to a CSV file\n",
    "with open('bookmarks.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header\n",
    "    writer.writerow(['Chapter', 'Page'])\n",
    "    # Write the bookmarks\n",
    "    for title, page in bookmarks:\n",
    "        writer.writerow([title, page])\n",
    "        \"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
